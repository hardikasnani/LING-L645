{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Given the abstract of a research paper, generate it's title"
      ],
      "metadata": {
        "id": "fe1rgWj2b5gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install the openAI library"
      ],
      "metadata": {
        "id": "ChPNpqsTfKbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JktOWGXpa3iD"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import all necessary libraries"
      ],
      "metadata": {
        "id": "scOXfzS0chB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import uuid\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "zaDH0LaLa69S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Use the shared api key"
      ],
      "metadata": {
        "id": "p1tFpfudehEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Insert your api key\n",
        "openai.api_key = \"<API-KEY>\""
      ],
      "metadata": {
        "id": "qBi9dvBVedaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create a class Example that will create the different input-output pairs for the GPT3 model"
      ],
      "metadata": {
        "id": "Xge4QCh6cvbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Example:\n",
        "\n",
        "   # Stores an input, output pair and formats it to prime the model\n",
        "   def __init__(self, input, output):\n",
        "       self.input = input\n",
        "       self.output = output\n",
        "       self.id = uuid.uuid4().hex\n",
        "\n",
        "   # To obtain the input provided for an example\n",
        "   def get_input(self):\n",
        "       return self.input\n",
        "\n",
        "   # To obtain the output provided for an example\n",
        "   def get_output(self):\n",
        "       return self.output\n",
        "\n",
        "   # To obtain the unique id of an example\n",
        "   def get_id(self):\n",
        "       return self.id"
      ],
      "metadata": {
        "id": "e8wY1V5Ja-Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Create a GPT-3 class that will manage different parameters of the GPT-3 model and create prompts for the model"
      ],
      "metadata": {
        "id": "upc3yjMqdhx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT3:\n",
        "\n",
        "   # initialises parameters and adds default values\n",
        "   def __init__(self, describe_task, engine='davinci', temperature=0.5, max_tokens=100,\n",
        "\n",
        "       input_prefix=\"input: \", input_suffix=\"\\n\", output_prefix=\"output: \",\n",
        "       output_suffix=\"\\n\\n\", append_output_prefix_to_query=False):\n",
        "       self.examples = {}\n",
        "       self.engine = engine\n",
        "       self.temperature = temperature\n",
        "       self.max_tokens = max_tokens\n",
        "       self.input_prefix = input_prefix\n",
        "       self.input_suffix = input_suffix\n",
        "       self.output_prefix = output_prefix\n",
        "       self.output_suffix = output_suffix\n",
        "       self.append_output_prefix_to_query = append_output_prefix_to_query\n",
        "       self.stop = (output_suffix + input_prefix).strip()\n",
        "       self.description = describe_task\n",
        "\n",
        "   # Adds an example to the model object. Example is an instance of the Example class.\n",
        "   def add_example(self, ex):\n",
        "       # TODO:\n",
        "       # Add a new example to the example dictionary using the id as the key and example object as the value\n",
        "       ########## INSERT YOUR CODE HERE ##########\n",
        "\n",
        "   # Converts all the examples to a particular format to prime the model.\n",
        "   def get_prime_text(self):\n",
        "       return \"\".join(\n",
        "           [self.format_example(ex) for ex in self.examples.values()])\n",
        "\n",
        "   # Creates a query for the API request\n",
        "   def craft_query(self, prompt):\n",
        "       #print(\"description in cratft query\",self.description)\n",
        "       q = self.description+self.get_prime_text(\n",
        "       ) + self.input_prefix + prompt + self.input_suffix\n",
        "\n",
        "       #print(q)\n",
        "       if self.append_output_prefix_to_query:\n",
        "           q = q + self.output_prefix\n",
        "       return q\n",
        "\n",
        "   # Calls the API using the Completion endpoint with the specified values of the parameters\n",
        "   def submit_request(self, prompt):\n",
        "       response = openai.Completion.create(engine=self.engine,\n",
        "                                           prompt=self.craft_query(prompt),\n",
        "                                           max_tokens=self.max_tokens,\n",
        "                                           temperature=self.temperature,\n",
        "                                           top_p=1,\n",
        "                                           n=1,\n",
        "                                           stream=False,\n",
        "                                           stop=self.stop)\n",
        "       return response\n",
        "\n",
        "   # Formats the input output pair with appropriate prefixes and suffixes\n",
        "   def format_example(self, ex):\n",
        "       return self.input_prefix + ex.get_input(\n",
        "       ) + self.input_suffix + self.output_prefix + ex.get_output(\n",
        "       ) + self.output_suffix"
      ],
      "metadata": {
        "id": "g6BVS6-AbQkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Create the object of GPT3 Class and "
      ],
      "metadata": {
        "id": "GDVKy6bmeCc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:\n",
        "# Use the following parameters:\n",
        "# 1. engine: text-davinci-002\n",
        "# 2. temperature: Use any value from 0 to 1\n",
        "# 3. describe_task = Give a one liner description of what you want the model to do or leave it blank\n",
        "# 4. Feel free to experiment with other parameters of the GPT Class (optional)\n",
        "\n",
        "gpt3 = ########## INSERT YOUR CODE HERE ##########"
      ],
      "metadata": {
        "id": "R7ikJU5obVnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Read data from the csv"
      ],
      "metadata": {
        "id": "hjmtl4Lhj3nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Upload the csv on Google Colab from https://github.com/hardikasnani/LING-L645/blob/main/GPT3%20Practical/Task%202/Title_Abstract.csv\n",
        "# and then read it below\n",
        "\n",
        "data = ########## INSERT YOUR CODE HERE ##########"
      ],
      "metadata": {
        "id": "cT8ahK4Aj08G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Zero/Few Shot Learnings and Evaluation"
      ],
      "metadata": {
        "id": "k8_VrpOf0L91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_number_of_shots = ########## INSERT YOUR CODE HERE ##########\n",
        "bleu_scores = []\n",
        "shots=[number_of_shots for number_of_shots in range(max_number_of_shots)]\n",
        "\n",
        "for number_shots in shots:\n",
        "  for index_in_csv in range(number_shots):\n",
        "      # TODO:\n",
        "      # Create example object by using the abstract and title from the csv and add that example to the gpt3 object\n",
        "      ########## INSERT YOUR CODE HERE ##########\n",
        "\n",
        "  # prompt for which the title has to be generated\n",
        "  prompt=\"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\"\n",
        "  \n",
        "  # TODO:\n",
        "  # Submit the prompt using the suitable function from the GPT3 class\n",
        "  response = ########## INSERT YOUR CODE HERE ##########\n",
        "\n",
        "  response_list = response.choices[0].text.lower().split(\" \")\n",
        "\n",
        "  print(\"Number of shots: \", number_shots)\n",
        "  print(response.choices[0].text) \n",
        "\n",
        "  # BLEU Score Calculations\n",
        "  references = [\"language models are few-shot learners\".split()]\n",
        "  candidate = response_list[1:]\n",
        "\n",
        "  # TODO:\n",
        "  # Calculate the BLEU Score using the references and canditate\n",
        "  ########## INSERT YOUR CODE HERE ##########\n",
        "\n",
        "  print(\"-------------------------------------------------\")\n",
        "  print(\"-------------------------------------------------\")\n",
        "\n",
        "  gpt3.examples={}"
      ],
      "metadata": {
        "id": "nrP8hJQSh2xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Plot"
      ],
      "metadata": {
        "id": "a_HoMQER0JCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot the graph using bleu_scores vs shots by replacing ??? in below code\n",
        "\n",
        "score_per_shot = pd.DataFrame({'Number_of_Shots' : ???, 'BLEU_Score' : ???})\n",
        "score_per_shot.sort_values(by='BLEU_Score', ascending=False)\n",
        "ax = sns.lineplot(data=[score_per_shot['BLEU_Score']], dashes=False, markers=True)\n",
        "ax.set_xticks(range(len(score_per_shot['Number_of_Shots'])))\n",
        "ax.set_xticklabels(score_per_shot['Number_of_Shots'])"
      ],
      "metadata": {
        "id": "om8Tl_zBbbSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Comment on your learnings"
      ],
      "metadata": {
        "id": "H20eT3Tt5Doj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### \\# TODO:\n",
        "\n",
        "Share your learnings here"
      ],
      "metadata": {
        "id": "Wk0nI-5p5G0s"
      }
    }
  ]
}