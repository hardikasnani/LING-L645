{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b5c35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def preprocess(s):\n",
    "    \"\"\"Tokenise a line\"\"\"\n",
    "    o = re.sub('([^a-zA-Z0-9\\']+)', ' \\g<1> ', s.strip())\n",
    "    return ['<BOS>'] + re.sub('  *', ' ', o).strip().split(' ')\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EMBEDDING_DIM = 4\n",
    "CONTEXT_SIZE = 2 #!!!#\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Trigram Neural Network Model\n",
    "class TrigramNNmodel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(TrigramNNmodel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of x1 and x2 embeddings\n",
    "        embeds = self.embeddings(inputs).view(\n",
    "                (-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18a8ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 2.7779366970062256\n",
      "Epoch: 1 loss: 2.3805863857269287\n",
      "Epoch: 2 loss: 1.9992704391479492\n",
      "Epoch: 3 loss: 1.7564469575881958\n",
      "Epoch: 4 loss: 1.6033422946929932\n",
      "Epoch: 5 loss: 1.4789083003997803\n",
      "Epoch: 6 loss: 1.3662440776824951\n",
      "Epoch: 7 loss: 1.2539446353912354\n",
      "Epoch: 8 loss: 1.1312713623046875\n",
      "Epoch: 9 loss: 1.0010528564453125\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "import sys, re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "training_samples = []\n",
    "vocabulary = set(['<UNK>'])\n",
    "training_file = open('train.txt', 'r')\n",
    "lines = training_file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    tokens = preprocess(line)\n",
    "    for i in tokens: vocabulary.add(i) \n",
    "    training_samples.append(tokens)\n",
    "\n",
    "word2idx = {k: v for v, k in enumerate(vocabulary)}\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "for tokens in training_samples:\n",
    "    for i in range(len(tokens) - 2): #!!!#\n",
    "        x_train.append([word2idx[tokens[i]],word2idx[tokens[i+1]]]) #!!!#\n",
    "        y_train.append([word2idx[tokens[i+2]]]) #!!!#\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "train_set = np.concatenate((x_train, y_train), axis=1)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, data_tensor in enumerate(train_loader):\n",
    "        context_tensor = data_tensor[:,0:2] #!!!#\n",
    "        target_tensor = data_tensor[:,2] #!!!#\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        log_probs = model(context_tensor)\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()    \n",
    "\n",
    "    print('Epoch:', epoch, 'loss:', float(loss))\n",
    "\n",
    "torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'model.lm')\n",
    "\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5245a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 10]\n",
      " [10 12]\n",
      " [12  3]]\n",
      "[[12]\n",
      " [ 3]\n",
      " [ 5]]\n",
      "0.019606\t-3.931920\t ['<BOS>', 'where', 'are', 'you', '?']\n",
      "[[ 2 11]\n",
      " [11  3]\n",
      " [ 3  0]\n",
      " [ 0 14]]\n",
      "[[ 3]\n",
      " [ 0]\n",
      " [14]\n",
      " [ 5]]\n",
      "0.022908\t-3.776251\t ['<BOS>', 'were', 'you', 'in', 'england', '?']\n",
      "[[ 2 12]\n",
      " [12  3]\n",
      " [ 3  0]\n",
      " [ 0  1]]\n",
      "[[3]\n",
      " [0]\n",
      " [1]\n",
      " [5]]\n",
      "0.024515\t-3.708455\t ['<BOS>', 'are', 'you', 'in', 'mexico', '?']\n",
      "[[ 2 15]\n",
      " [15  7]\n",
      " [ 7  0]\n",
      " [ 0  1]]\n",
      "[[7]\n",
      " [0]\n",
      " [1]\n",
      " [8]]\n",
      "0.000025\t-10.586203\t ['<BOS>', 'i', 'am', 'in', 'mexico', '.']\n",
      "[[ 2 12]\n",
      " [12  3]\n",
      " [ 3  4]\n",
      " [ 4  0]\n",
      " [ 0  1]]\n",
      "[[3]\n",
      " [4]\n",
      " [0]\n",
      " [1]\n",
      " [5]]\n",
      "0.000015\t-11.103380\t ['<BOS>', 'are', 'you', 'still', 'in', 'mexico', '?']\n"
     ]
    }
   ],
   "source": [
    "import sys, re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "blob = torch.load('model.lm')\n",
    "idx2word = blob['vocab']\n",
    "word2idx = {k: v for v, k in idx2word.items()}\n",
    "vocabulary = set(idx2word.values())\n",
    "\n",
    "model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
    "model.load_state_dict(blob['model'])\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "testing_file = open('test.txt', 'r')\n",
    "lines = testing_file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    tokens = preprocess(line)\n",
    "    \n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    for i in range(len(tokens) - 2): #!!!#\n",
    "        x_test.append([word2idx[tokens[i]],word2idx[tokens[i+1]]]) #!!!#\n",
    "        y_test.append([word2idx[tokens[i+2]]]) #!!!#\n",
    "    \n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    print(x_test)\n",
    "    print(y_test)\n",
    "    \n",
    "    test_set = np.concatenate((x_test, y_test), axis=1)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    total_prob = 1.0\n",
    "    for i, data_tensor in enumerate(test_loader):\n",
    "        context_tensor = data_tensor[:,0:2] #!!!#\n",
    "        target_tensor = data_tensor[:,2] #!!!#\n",
    "        log_probs = model(context_tensor)\n",
    "        probs = torch.exp(log_probs)\n",
    "        predicted_label = int(torch.argmax(probs, dim=1)[0])\n",
    "    \n",
    "        true_label = y_test[i][0]\n",
    "        true_word = idx2word[true_label]\n",
    "    \n",
    "        prob_true = float(probs[0][true_label])\n",
    "        total_prob *= prob_true\n",
    "    \n",
    "    print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)\n",
    "    \n",
    "    # line = sys.stdin.readline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
