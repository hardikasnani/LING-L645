{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5c35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def preprocess(s):\n",
    "    \"\"\"Tokenise a line\"\"\"\n",
    "    o = re.sub('([^a-zA-Z0-9\\']+)', ' \\g<1> ', s.strip())\n",
    "    return ['<BOS>'] + re.sub('  *', ' ', o).strip().split(' ')\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EMBEDDING_DIM = 4\n",
    "CONTEXT_SIZE = 1 #!!!#\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Bigram Neural Network Model\n",
    "class BigramNNmodel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(BigramNNmodel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of x1 and x2 embeddings\n",
    "        embeds = self.embeddings(inputs).view(\n",
    "                (-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a8ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 2.669884204864502\n",
      "Epoch: 1 loss: 2.5198047161102295\n",
      "Epoch: 2 loss: 2.3476340770721436\n",
      "Epoch: 3 loss: 2.1197338104248047\n",
      "Epoch: 4 loss: 1.832275152206421\n",
      "Epoch: 5 loss: 1.5311470031738281\n",
      "Epoch: 6 loss: 1.271499752998352\n",
      "Epoch: 7 loss: 1.0723257064819336\n",
      "Epoch: 8 loss: 0.92421954870224\n",
      "Epoch: 9 loss: 0.814369261264801\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "import sys, re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from model import *\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "training_samples = []\n",
    "vocabulary = set(['<UNK>'])\n",
    "training_file = open('train.txt','r')\n",
    "lines = training_file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    tokens = preprocess(line)\n",
    "    for i in tokens: vocabulary.add(i)\n",
    "    training_samples.append(tokens)\n",
    "\n",
    "word2idx = {k: v for v, k in enumerate(vocabulary)}\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "for tokens in training_samples:\n",
    "    for i in range(len(tokens) - 1): #!!!#\n",
    "        x_train.append([word2idx[tokens[i]]]) #!!!#\n",
    "        y_train.append([word2idx[tokens[i+1]]]) #!!!#\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "train_set = np.concatenate((x_train, y_train), axis=1)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, data_tensor in enumerate(train_loader):\n",
    "        context_tensor = data_tensor[:,0:1] #!!!#\n",
    "        target_tensor = data_tensor[:,1] #!!!#\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        log_probs = model(context_tensor)\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    print('Epoch:', epoch, 'loss:', float(loss))\n",
    "\n",
    "torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'model.lm')\n",
    "\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5245a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12]\n",
      " [ 4]\n",
      " [14]\n",
      " [10]]\n",
      "[[ 4]\n",
      " [14]\n",
      " [10]\n",
      " [ 3]]\n",
      "0.002616\t-5.946220\t ['<BOS>', 'where', 'are', 'you', '?']\n",
      "[[12]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [ 8]]\n",
      "[[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [ 8]\n",
      " [ 3]]\n",
      "0.000680\t-7.292821\t ['<BOS>', 'were', 'you', 'in', 'england', '?']\n",
      "[[12]\n",
      " [14]\n",
      " [10]\n",
      " [11]\n",
      " [ 2]]\n",
      "[[14]\n",
      " [10]\n",
      " [11]\n",
      " [ 2]\n",
      " [ 3]]\n",
      "0.010468\t-4.559388\t ['<BOS>', 'are', 'you', 'in', 'mexico', '?']\n",
      "[[12]\n",
      " [15]\n",
      " [ 6]\n",
      " [11]\n",
      " [ 2]]\n",
      "[[15]\n",
      " [ 6]\n",
      " [11]\n",
      " [ 2]\n",
      " [13]]\n",
      "0.000062\t-9.692129\t ['<BOS>', 'i', 'am', 'in', 'mexico', '.']\n",
      "[[12]\n",
      " [14]\n",
      " [10]\n",
      " [ 0]\n",
      " [11]\n",
      " [ 2]]\n",
      "[[14]\n",
      " [10]\n",
      " [ 0]\n",
      " [11]\n",
      " [ 2]\n",
      " [ 3]]\n",
      "0.000161\t-8.731253\t ['<BOS>', 'are', 'you', 'still', 'in', 'mexico', '?']\n"
     ]
    }
   ],
   "source": [
    "import sys, re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from model import *\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "blob = torch.load('model.lm')\n",
    "idx2word = blob['vocab']\n",
    "word2idx = {k: v for v, k in idx2word.items()}\n",
    "vocabulary = set(idx2word.values())\n",
    "\n",
    "model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
    "model.load_state_dict(blob['model'])\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "testing_file = open('test.txt','r')\n",
    "lines = testing_file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    tokens = preprocess(line)\n",
    "\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    for i in range(len(tokens) - 1):  # !!!#\n",
    "        x_test.append([word2idx[tokens[i]]])  # !!!#\n",
    "        y_test.append([word2idx[tokens[i + 1]]])  # !!!#\n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    print(x_test)\n",
    "    print(y_test)\n",
    "\n",
    "    test_set = np.concatenate((x_test, y_test), axis=1)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "    total_prob = 1.0\n",
    "    for i, data_tensor in enumerate(test_loader):\n",
    "        context_tensor = data_tensor[:, 0:1]  # !!!#\n",
    "        log_probs = model(context_tensor)\n",
    "        probs = torch.exp(log_probs)\n",
    "\n",
    "        true_label = y_test[i][0]\n",
    "        true_word = idx2word[true_label]\n",
    "\n",
    "        prob_true = float(probs[0][true_label])\n",
    "        total_prob *= prob_true\n",
    "\n",
    "    print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)\n",
    "\n",
    "    line = sys.stdin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b08ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
